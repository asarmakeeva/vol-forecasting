{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Exploratory Data Analysis for Volatility Forecasting\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis to understand:\n",
    "- Time series properties of financial returns\n",
    "- Autocorrelation structure (ACF, PACF)\n",
    "- Optimal lag selection for forecasting\n",
    "- Stylized facts (volatility clustering, leverage effects, heavy tails)\n",
    "- Data quality and preprocessing needs\n",
    "\n",
    "**Key Output:** Identification of important lags: [1, 2, 6, 11, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.config import *\n",
    "from src.data.features import garman_klass, realized_vol_from_daily\n",
    "\n",
    "# Set random seed\n",
    "set_seeds()\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use(PLOT_STYLE)\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"✓ Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Download data\n",
    "ticker = DEFAULT_TICKER\n",
    "df = yf.download(ticker, start=DEFAULT_START, end=DEFAULT_END, progress=False)\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "# Calculate returns\n",
    "df['returns'] = df['close'].pct_change()\n",
    "df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "\n",
    "# Realized volatility\n",
    "df['rv'] = realized_vol_from_daily(df)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Data: {ticker}\")\n",
    "print(f\"Period: {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "print(f\"Observations: {len(df)}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "returns = df['returns'].dropna()\n",
    "\n",
    "stats_dict = {\n",
    "    'Mean (daily)': returns.mean(),\n",
    "    'Mean (annualized %)': returns.mean() * TRADING_DAYS * 100,\n",
    "    'Std (daily)': returns.std(),\n",
    "    'Std (annualized %)': returns.std() * np.sqrt(TRADING_DAYS) * 100,\n",
    "    'Skewness': stats.skew(returns),\n",
    "    'Kurtosis': stats.kurtosis(returns),\n",
    "    'Min': returns.min(),\n",
    "    'Max': returns.max(),\n",
    "    'Sharpe (annualized)': (returns.mean() / returns.std()) * np.sqrt(TRADING_DAYS)\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats_dict, index=['Value']).T\n",
    "print(\"\\n=== RETURN STATISTICS ===\")\n",
    "print(stats_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "# Price\n",
    "axes[0].plot(df.index, df['close'], linewidth=1)\n",
    "axes[0].set_title(f'{ticker} Price', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Returns\n",
    "axes[1].plot(df.index, df['returns'] * 100, linewidth=0.5, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[1].set_title('Daily Returns', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Return (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute returns (volatility proxy)\n",
    "axes[2].plot(df.index, np.abs(df['returns']) * 100, linewidth=0.5, color='orange', alpha=0.7)\n",
    "axes[2].set_title('Absolute Returns (Volatility Proxy)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('|Return| (%)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Realized volatility\n",
    "axes[3].plot(df.index, df['rv'] * 100, linewidth=1, color='red')\n",
    "axes[3].set_title('Realized Volatility (Garman-Klass)', fontsize=12, fontweight='bold')\n",
    "axes[3].set_ylabel('Volatility (% ann.)')\n",
    "axes[3].set_xlabel('Date')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(returns, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "\n",
    "# Overlay normal distribution\n",
    "mu, sigma = returns.mean(), returns.std()\n",
    "x = np.linspace(returns.min(), returns.max(), 100)\n",
    "axes[0, 0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal')\n",
    "axes[0, 0].set_title('Return Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Return')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(returns, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot vs Normal', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Log returns histogram\n",
    "log_returns = df['log_returns'].dropna()\n",
    "axes[1, 0].hist(log_returns, bins=50, density=True, alpha=0.7, color='green', edgecolor='black')\n",
    "mu_log, sigma_log = log_returns.mean(), log_returns.std()\n",
    "x_log = np.linspace(log_returns.min(), log_returns.max(), 100)\n",
    "axes[1, 0].plot(x_log, stats.norm.pdf(x_log, mu_log, sigma_log), 'r-', linewidth=2, label='Normal')\n",
    "axes[1, 0].set_title('Log Return Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Log Return')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by year\n",
    "df_box = df[['returns']].copy()\n",
    "df_box['year'] = df_box.index.year\n",
    "df_box.boxplot(column='returns', by='year', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Returns by Year', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Year')\n",
    "axes[1, 1].set_ylabel('Return')\n",
    "axes[1, 1].get_figure().suptitle('')  # Remove auto title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "jb_stat, jb_pval = stats.jarque_bera(returns)\n",
    "print(\"\\n=== NORMALITY TEST ===\")\n",
    "print(f\"Jarque-Bera statistic: {jb_stat:.2f}\")\n",
    "print(f\"P-value: {jb_pval:.6f}\")\n",
    "if jb_pval < 0.05:\n",
    "    print(\"✓ Reject normality (heavy tails detected)\")\n",
    "else:\n",
    "    print(\"Cannot reject normality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autocorrelation Analysis (ACF & PACF)\n",
    "\n",
    "### Key for Lag Selection\n",
    "\n",
    "We'll examine:\n",
    "- **ACF (Autocorrelation)** - Linear dependence at different lags\n",
    "- **PACF (Partial Autocorrelation)** - Direct effect of each lag (controlling for others)\n",
    "- **Squared returns** - Volatility clustering\n",
    "- **Absolute returns** - Alternative volatility measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "max_lags = 40\n",
    "\n",
    "# ACF of returns\n",
    "plot_acf(returns, lags=max_lags, ax=axes[0, 0], alpha=0.05)\n",
    "axes[0, 0].set_title('ACF: Returns', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Lag')\n",
    "\n",
    "# PACF of returns\n",
    "plot_pacf(returns, lags=max_lags, ax=axes[0, 1], alpha=0.05, method='ywm')\n",
    "axes[0, 1].set_title('PACF: Returns', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Lag')\n",
    "\n",
    "# ACF of squared returns (volatility clustering)\n",
    "returns_sq = returns ** 2\n",
    "plot_acf(returns_sq, lags=max_lags, ax=axes[1, 0], alpha=0.05)\n",
    "axes[1, 0].set_title('ACF: Squared Returns (Volatility Clustering)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Lag')\n",
    "\n",
    "# PACF of squared returns\n",
    "plot_pacf(returns_sq, lags=max_lags, ax=axes[1, 1], alpha=0.05, method='ywm')\n",
    "axes[1, 1].set_title('PACF: Squared Returns', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ACF and PACF values to identify significant lags\n",
    "acf_vals = acf(returns, nlags=max_lags, fft=False)\n",
    "pacf_vals = pacf(returns, nlags=max_lags, method='ywm')\n",
    "\n",
    "# 95% confidence interval\n",
    "conf_interval = 1.96 / np.sqrt(len(returns))\n",
    "\n",
    "# Find significant lags (beyond confidence interval)\n",
    "significant_acf = np.where(np.abs(acf_vals[1:]) > conf_interval)[0] + 1\n",
    "significant_pacf = np.where(np.abs(pacf_vals[1:]) > conf_interval)[0] + 1\n",
    "\n",
    "print(\"\\n=== SIGNIFICANT LAGS ===\")\n",
    "print(f\"\\nACF significant lags (|corr| > {conf_interval:.3f}):\")\n",
    "print(f\"  {significant_acf[:20]}\")\n",
    "print(f\"\\nPACF significant lags:\")\n",
    "print(f\"  {significant_pacf[:20]}\")\n",
    "\n",
    "# Top PACF lags by absolute value\n",
    "top_pacf_indices = np.argsort(np.abs(pacf_vals[1:]))[::-1][:10] + 1\n",
    "top_pacf_values = pacf_vals[top_pacf_indices]\n",
    "\n",
    "print(f\"\\n=== TOP 10 PACF LAGS (by absolute value) ===\")\n",
    "for lag, val in zip(top_pacf_indices, top_pacf_values):\n",
    "    print(f\"  Lag {lag:2d}: {val:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Volatility Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ljung-Box test for autocorrelation\n",
    "lb_returns = acorr_ljungbox(returns, lags=[10, 20, 30], return_df=True)\n",
    "lb_squared = acorr_ljungbox(returns_sq, lags=[10, 20, 30], return_df=True)\n",
    "\n",
    "print(\"=== LJUNG-BOX TEST ===\")\n",
    "print(\"\\nReturns:\")\n",
    "print(lb_returns[['lb_stat', 'lb_pvalue']])\n",
    "print(\"\\nSquared Returns (Volatility Clustering):\")\n",
    "print(lb_squared[['lb_stat', 'lb_pvalue']])\n",
    "\n",
    "if lb_squared['lb_pvalue'].iloc[0] < 0.05:\n",
    "    print(\"\\n✓ Volatility clustering detected (p < 0.05)\")\n",
    "    print(\"  → GARCH-type models appropriate\")\n",
    "else:\n",
    "    print(\"\\n✗ No significant volatility clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Leverage Effect Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between returns and future volatility\n",
    "max_lag = 20\n",
    "lag_corr = []\n",
    "\n",
    "for lag in range(1, max_lag + 1):\n",
    "    corr = df['returns'].corr(df['rv'].shift(-lag))\n",
    "    lag_corr.append(corr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(range(1, max_lag + 1), lag_corr, alpha=0.7, color='steelblue')\n",
    "ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "ax.set_xlabel('Lag (days)')\n",
    "ax.set_ylabel('Correlation')\n",
    "ax.set_title('Leverage Effect: Correlation(Return_t, Vol_t+lag)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== LEVERAGE EFFECT ===\")\n",
    "print(f\"Corr(return_t, vol_t+1): {lag_corr[0]:.4f}\")\n",
    "if lag_corr[0] < -0.1:\n",
    "    print(\"✓ Strong leverage effect detected\")\n",
    "    print(\"  → EGARCH recommended (captures asymmetry)\")\n",
    "elif lag_corr[0] < -0.05:\n",
    "    print(\"✓ Moderate leverage effect\")\n",
    "else:\n",
    "    print(\"✗ Weak leverage effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lag Selection for Forecasting Models\n",
    "\n",
    "Based on PACF analysis and domain knowledge, we select lags that:\n",
    "1. Show significant partial correlation\n",
    "2. Are practically meaningful (1-day, weekly, bi-weekly)\n",
    "3. Avoid multicollinearity issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lagged return predictiveness\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create lagged features\n",
    "max_test_lag = 30\n",
    "X_lags = pd.DataFrame(index=df.index)\n",
    "\n",
    "for lag in range(1, max_test_lag + 1):\n",
    "    X_lags[f'lag_{lag}'] = df['returns'].shift(lag)\n",
    "\n",
    "# Target: future absolute returns (volatility proxy)\n",
    "y = np.abs(df['returns'])\n",
    "\n",
    "# Align data\n",
    "data = pd.concat([X_lags, y.rename('target')], axis=1).dropna()\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Lasso for feature selection\n",
    "lasso = LassoCV(cv=5, random_state=RANDOM_SEED, max_iter=10000)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get important lags\n",
    "coefficients = pd.Series(lasso.coef_, index=X.columns)\n",
    "important_lags_lasso = coefficients[coefficients != 0].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"=== LASSO FEATURE SELECTION ===\")\n",
    "print(f\"\\nAlpha (selected): {lasso.alpha_:.6f}\")\n",
    "print(f\"\\nTop 10 most important lags:\")\n",
    "for lag, coef in important_lags_lasso.head(10).items():\n",
    "    lag_num = int(lag.split('_')[1])\n",
    "    print(f\"  Lag {lag_num:2d}: |coef| = {coef:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of lag importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# PACF values\n",
    "axes[0].bar(range(1, 21), np.abs(pacf_vals[1:21]), alpha=0.7, color='steelblue')\n",
    "axes[0].axhline(y=conf_interval, color='red', linestyle='--', linewidth=1, label='95% CI')\n",
    "axes[0].set_xlabel('Lag')\n",
    "axes[0].set_ylabel('|PACF|')\n",
    "axes[0].set_title('Lag Importance: PACF', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Lasso coefficients\n",
    "lasso_top20 = coefficients.abs().sort_values(ascending=False).head(20)\n",
    "lag_nums = [int(l.split('_')[1]) for l in lasso_top20.index]\n",
    "axes[1].bar(lag_nums, lasso_top20.values, alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('Lag')\n",
    "axes[1].set_ylabel('|Lasso Coefficient|')\n",
    "axes[1].set_title('Lag Importance: Lasso', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Lag Selection\n",
    "\n",
    "Based on:\n",
    "1. **PACF analysis** - Significant partial correlations\n",
    "2. **Lasso selection** - Predictive power for volatility\n",
    "3. **Practical considerations** - Interpretability and meaningful time scales\n",
    "\n",
    "### Selected Lags: [1, 2, 6, 11, 16]\n",
    "\n",
    "Rationale:\n",
    "- **Lag 1, 2**: Recent momentum (1-2 days)\n",
    "- **Lag 6**: Weekly effect (~1 week)\n",
    "- **Lag 11, 16**: Bi-weekly patterns (~2-3 weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate selected lags\n",
    "SELECTED_LAGS = [1, 2, 6, 11, 16]\n",
    "\n",
    "print(\"=== VALIDATION OF SELECTED LAGS ===\")\n",
    "print(f\"\\nSelected lags: {SELECTED_LAGS}\")\n",
    "print(\"\\nPACF values:\")\n",
    "for lag in SELECTED_LAGS:\n",
    "    print(f\"  Lag {lag:2d}: {pacf_vals[lag]:+.4f} {'*' if np.abs(pacf_vals[lag]) > conf_interval else ''}\")\n",
    "\n",
    "print(\"\\nLasso coefficients:\")\n",
    "for lag in SELECTED_LAGS:\n",
    "    coef = coefficients[f'lag_{lag}']\n",
    "    print(f\"  Lag {lag:2d}: {coef:+.6f}\")\n",
    "\n",
    "print(\"\\n✓ These lags will be used in feature engineering (src/config.py)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stationarity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dickey-Fuller test\n",
    "adf_result = adfuller(returns, autolag='AIC')\n",
    "\n",
    "print(\"=== AUGMENTED DICKEY-FULLER TEST ===\")\n",
    "print(f\"\\nTest for: Returns\")\n",
    "print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"P-value: {adf_result[1]:.6f}\")\n",
    "print(f\"Lags used: {adf_result[2]}\")\n",
    "print(\"\\nCritical values:\")\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f\"  {key}: {value:.3f}\")\n",
    "\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"\\n✓ Returns are stationary (p < 0.05)\")\n",
    "    print(\"  → Can use for modeling\")\n",
    "else:\n",
    "    print(\"\\n⚠ Returns may not be stationary\")\n",
    "    print(\"  → Consider differencing or transformations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Distribution Properties:**\n",
    "   - Returns exhibit heavy tails (excess kurtosis)\n",
    "   - Not normally distributed (Jarque-Bera test rejects normality)\n",
    "   - Student-t distribution more appropriate\n",
    "\n",
    "2. **Time Series Properties:**\n",
    "   - Returns are stationary (ADF test)\n",
    "   - Weak autocorrelation in returns\n",
    "   - **Strong autocorrelation in squared/absolute returns → Volatility clustering**\n",
    "\n",
    "3. **Leverage Effect:**\n",
    "   - Negative correlation between returns and future volatility\n",
    "   - Asymmetric models (EGARCH, GJR-GARCH) recommended\n",
    "\n",
    "4. **Optimal Lags for Forecasting:**\n",
    "   - **[1, 2, 6, 11, 16]** identified through PACF and Lasso analysis\n",
    "   - Capture short-term (1-2 days) and medium-term (1-3 weeks) dynamics\n",
    "   - Balance between information and multicollinearity\n",
    "\n",
    "### Implications for Modeling:\n",
    "\n",
    "**GARCH Models:**\n",
    "- ✓ Volatility clustering confirms GARCH is appropriate\n",
    "- ✓ Use EGARCH to capture leverage effects\n",
    "- ✓ Consider Student-t distribution for heavy tails\n",
    "\n",
    "**LSTM/Deep Learning:**\n",
    "- ✓ Use lags [1, 2, 6, 11, 16] as key features\n",
    "- ✓ 30-day sequence length (LSTM_SEQ_LEN) captures relevant history\n",
    "- ✓ Non-linear patterns suggest potential for ML models\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Notebook 02:** Implement GARCH/EGARCH baselines\n",
    "2. **Notebook 03:** Train LSTM with selected lags\n",
    "3. **Notebook 04:** Compare forecasts and backtests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key findings for reference\n",
    "findings = {\n",
    "    'ticker': ticker,\n",
    "    'period': f\"{df.index[0].date()} to {df.index[-1].date()}\",\n",
    "    'n_observations': len(df),\n",
    "    'selected_lags': SELECTED_LAGS,\n",
    "    'volatility_clustering': lb_squared['lb_pvalue'].iloc[0] < 0.05,\n",
    "    'leverage_effect': lag_corr[0],\n",
    "    'stationary': adf_result[1] < 0.05,\n",
    "    'heavy_tails': jb_pval < 0.05,\n",
    "    'mean_return_annual': returns.mean() * TRADING_DAYS * 100,\n",
    "    'volatility_annual': returns.std() * np.sqrt(TRADING_DAYS) * 100\n",
    "}\n",
    "\n",
    "print(\"\\n=== EXPLORATORY ANALYSIS COMPLETE ===\")\n",
    "print(\"\\nKey findings saved to 'findings' dictionary\")\n",
    "print(\"\\nProceed to:\")\n",
    "print(\"  - notebooks/02_garch_baselines.ipynb for GARCH models\")\n",
    "print(\"  - notebooks/03_lstm_transformer.ipynb for deep learning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
