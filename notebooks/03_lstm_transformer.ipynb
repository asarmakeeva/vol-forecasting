{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: LSTM and Transformer Models for Volatility Forecasting\n",
    "\n",
    "This notebook implements and trains deep learning models:\n",
    "- LSTM with attention to sequences\n",
    "- Feature engineering based on lag analysis from Notebook 01\n",
    "- Walk-forward validation\n",
    "- Comparison with GARCH baselines from Notebook 02\n",
    "\n",
    "**Key Innovation:** Using research-validated lags [1, 2, 6, 11, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.config import *\n",
    "from src.data.features import create_volatility_features, select_lstm_features, normalize_features\n",
    "from src.models.lstm import LSTMVol, LSTMVolTrainer, create_sequences, prepare_dataloaders\n",
    "from src.eval.metrics import qlike\n",
    "\n",
    "# Set seeds\n",
    "set_seeds()\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting\n",
    "plt.style.use(PLOT_STYLE)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Environment loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Download data\n",
    "ticker = DEFAULT_TICKER\n",
    "df_raw = yf.download(ticker, start=DEFAULT_START, end=DEFAULT_END, progress=False)\n",
    "df_raw.columns = [c.lower() for c in df_raw.columns]\n",
    "\n",
    "print(f\"Downloaded {len(df_raw)} days of {ticker} data\")\n",
    "print(f\"Period: {df_raw.index[0].date()} to {df_raw.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive features\n",
    "print(\"Creating features...\")\n",
    "features_df = create_volatility_features(df_raw)\n",
    "\n",
    "print(f\"\\n✓ Created {len(features_df.columns)} features\")\n",
    "print(f\"✓ {len(features_df)} valid observations\")\n",
    "print(f\"\\nFeatures: {list(features_df.columns[:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (remove highly correlated)\n",
    "feature_cols = select_lstm_features(features_df, correlation_threshold=CORRELATION_THRESHOLD)\n",
    "\n",
    "print(f\"\\n✓ Selected {len(feature_cols)} features after correlation filtering\")\n",
    "print(f\"\\nSelected features: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import train_test_split_by_date\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split_by_date(features_df, TRAIN_END)\n",
    "\n",
    "print(f\"Training: {train_df.index[0].date()} to {train_df.index[-1].date()} ({len(train_df)} days)\")\n",
    "print(f\"Testing:  {test_df.index[0].date()} to {test_df.index[-1].date()} ({len(test_df)} days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "train_features, test_features, scaler_params = normalize_features(\n",
    "    train_df[feature_cols],\n",
    "    test_df[feature_cols],\n",
    "    method='standardize'\n",
    ")\n",
    "\n",
    "print(\"✓ Features normalized (z-score using training statistics)\")\n",
    "print(f\"\\nMean of training features: {train_features.mean().mean():.6f}\")\n",
    "print(f\"Std of training features: {train_features.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sequences for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "train_data = train_df.copy()\n",
    "train_data[feature_cols] = train_features\n",
    "\n",
    "test_data = test_df.copy()\n",
    "test_data[feature_cols] = test_features\n",
    "\n",
    "# Create sequences\n",
    "X_train, y_train, train_dates = create_sequences(\n",
    "    train_data,\n",
    "    target_col='rv',\n",
    "    feature_cols=feature_cols,\n",
    "    seq_len=LSTM_SEQ_LEN,\n",
    "    forecast_horizon=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining sequences: {X_train.shape}\")\n",
    "print(f\"  Shape: (n_samples={X_train.shape[0]}, seq_len={X_train.shape[1]}, n_features={X_train.shape[2]})\")\n",
    "print(f\"\\nTraining targets: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train/val\n",
    "n_train = int(len(X_train) * (1 - LSTM_VAL_SPLIT))\n",
    "\n",
    "X_tr, y_tr = X_train[:n_train], y_train[:n_train]\n",
    "X_val, y_val = X_train[n_train:], y_train[n_train:]\n",
    "\n",
    "print(f\"Train split: {X_tr.shape[0]} samples\")\n",
    "print(f\"Val split:   {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = LSTMVol(\n",
    "    n_feats=len(feature_cols),\n",
    "    hidden=LSTM_HIDDEN,\n",
    "    layers=LSTM_LAYERS,\n",
    "    dropout=LSTM_DROPOUT\n",
    ")\n",
    "\n",
    "print(\"=== LSTM MODEL ===\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_loader, val_loader = prepare_dataloaders(\n",
    "    X_tr, y_tr,\n",
    "    X_val, y_val,\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "    shuffle_train=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches:   {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = LSTMVolTrainer(\n",
    "    model,\n",
    "    lr=LSTM_LR,\n",
    "    weight_decay=LSTM_WEIGHT_DECAY,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"  Device: {trainer.device}\")\n",
    "print(f\"  Learning rate: {LSTM_LR}\")\n",
    "print(f\"  Weight decay: {LSTM_WEIGHT_DECAY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"\\n=== TRAINING LSTM ===\")\n",
    "print(\"This will take a few minutes...\\n\")\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=LSTM_EPOCHS,\n",
    "    loss_fn='qlike',\n",
    "    early_stopping_patience=LSTM_PATIENCE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "ax.plot(epochs, history['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax.plot(epochs, history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('QLIKE Loss')\n",
    "ax.set_title('LSTM Training History', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark best epoch\n",
    "if hasattr(trainer, 'best_state'):\n",
    "    best_epoch = trainer.best_state['epoch'] + 1\n",
    "    ax.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.5, label=f'Best (epoch {best_epoch})')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final val loss:   {history['val_loss'][-1]:.4f}\")\n",
    "if hasattr(trainer, 'best_state'):\n",
    "    print(f\"Best val loss:    {trainer.best_state['val_loss']:.4f} (epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. In-Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "val_predictions = trainer.predict(torch.FloatTensor(X_val))\n",
    "\n",
    "# Compare with actuals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Time series\n",
    "val_dates = train_dates[n_train:]\n",
    "axes[0].plot(val_dates, y_val * 100, label='Actual', linewidth=1.5, alpha=0.8)\n",
    "axes[0].plot(val_dates, val_predictions * 100, label='Predicted', linewidth=1.5, alpha=0.8)\n",
    "axes[0].set_title('Validation Set: Actual vs Predicted Volatility', fontweight='bold')\n",
    "axes[0].set_ylabel('Volatility (% ann.)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter\n",
    "axes[1].scatter(val_predictions * 100, y_val * 100, alpha=0.5, s=20)\n",
    "axes[1].plot([0, y_val.max() * 100], [0, y_val.max() * 100], 'r--', linewidth=2, label='Perfect forecast')\n",
    "axes[1].set_xlabel('Predicted Volatility (% ann.)')\n",
    "axes[1].set_ylabel('Actual Volatility (% ann.)')\n",
    "axes[1].set_title('Scatter: Actual vs Predicted', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "mae = mean_absolute_error(y_val, val_predictions)\n",
    "r2 = r2_score(y_val, val_predictions)\n",
    "qlike_val = qlike(y_val, val_predictions)\n",
    "\n",
    "print(\"\\n=== VALIDATION METRICS ===\")\n",
    "print(f\"RMSE:  {rmse:.4f}\")\n",
    "print(f\"MAE:   {mae:.4f}\")\n",
    "print(f\"R²:    {r2:.4f}\")\n",
    "print(f\"QLIKE: {qlike_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Out-of-Sample Testing (Walk-Forward)\n",
    "\n",
    "For production forecasting, we need walk-forward validation with periodic refitting.\n",
    "This is computationally expensive, so we'll demonstrate the concept with a few refits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.lstm import rolling_lstm_forecast\n",
    "\n",
    "print(\"\\n=== OUT-OF-SAMPLE FORECASTING ===\")\n",
    "print(\"Generating rolling forecasts with monthly refitting...\")\n",
    "print(\"⚠ This will take 15-30 minutes!\\n\")\n",
    "\n",
    "# Combine all data\n",
    "all_data = features_df.copy()\n",
    "all_data[feature_cols] = pd.concat([\n",
    "    pd.DataFrame(train_features, index=train_df.index, columns=feature_cols),\n",
    "    pd.DataFrame(test_features, index=test_df.index, columns=feature_cols)\n",
    "])\n",
    "\n",
    "# Generate forecasts\n",
    "lstm_forecasts = rolling_lstm_forecast(\n",
    "    data=all_data,\n",
    "    target_col='rv',\n",
    "    feature_cols=feature_cols,\n",
    "    seq_len=LSTM_SEQ_LEN,\n",
    "    train_window=LSTM_TRAIN_WINDOW,\n",
    "    refit_freq=LSTM_REFIT_FREQ,\n",
    "    lstm_hidden=LSTM_HIDDEN,\n",
    "    lstm_layers=LSTM_LAYERS,\n",
    "    epochs=30,  # Reduced for speed\n",
    "    batch_size=LSTM_BATCH_SIZE,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(lstm_forecasts)} out-of-sample forecasts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to test period\n",
    "test_forecasts = lstm_forecasts[lstm_forecasts.index >= TEST_START]\n",
    "test_actuals = features_df.loc[test_forecasts.index, 'rv']\n",
    "\n",
    "print(f\"\\nTest period forecasts: {len(test_forecasts)}\")\n",
    "print(f\"Coverage: {test_forecasts.index[0].date()} to {test_forecasts.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretation\n",
    "\n",
    "### Feature Importance via Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature importance: permutation test\n",
    "baseline_loss = trainer.validate(val_loader, loss_fn='qlike')\n",
    "\n",
    "feature_importance = {}\n",
    "\n",
    "print(\"\\n=== FEATURE IMPORTANCE (Permutation Test) ===\")\n",
    "print(\"Testing each feature...\\n\")\n",
    "\n",
    "for i, feat_name in enumerate(feature_cols[:10]):  # Test first 10 features\n",
    "    # Permute feature\n",
    "    X_perm = X_val.copy()\n",
    "    np.random.shuffle(X_perm[:, :, i])\n",
    "    \n",
    "    # Create loader\n",
    "    perm_dataset = TensorDataset(torch.FloatTensor(X_perm), torch.FloatTensor(y_val))\n",
    "    perm_loader = DataLoader(perm_dataset, batch_size=LSTM_BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    perm_loss = trainer.validate(perm_loader, loss_fn='qlike')\n",
    "    importance = perm_loss - baseline_loss\n",
    "    \n",
    "    feature_importance[feat_name] = importance\n",
    "    print(f\"{feat_name:20s}: {importance:+.6f}\")\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = pd.Series(feature_importance).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "importance_df.plot(kind='barh', ax=ax, color='steelblue', alpha=0.7)\n",
    "ax.set_xlabel('Importance (Δ Loss when permuted)')\n",
    "ax.set_title('Top 10 Feature Importance', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Higher value = more important feature\")\n",
    "print(\"(Loss increases more when feature is permuted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Key Achievements:\n",
    "\n",
    "1. ✓ Created 40+ engineered features\n",
    "2. ✓ Selected features using correlation filtering\n",
    "3. ✓ Built LSTM model with {LSTM_HIDDEN} hidden units, {LSTM_LAYERS} layers\n",
    "4. ✓ Trained with QLIKE loss and early stopping\n",
    "5. ✓ Generated out-of-sample forecasts with walk-forward validation\n",
    "6. ✓ Analyzed feature importance\n",
    "\n",
    "### Model Configuration:\n",
    "\n",
    "- **Sequence length:** {LSTM_SEQ_LEN} days\n",
    "- **Features:** {len(feature_cols)} (including important lags [1,2,6,11,16])\n",
    "- **Architecture:** {LSTM_LAYERS}-layer LSTM with {LSTM_HIDDEN} hidden units\n",
    "- **Regularization:** Dropout ({LSTM_DROPOUT}), weight decay ({LSTM_WEIGHT_DECAY})\n",
    "- **Training:** Early stopping (patience={LSTM_PATIENCE}), LR scheduling\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Proceed to **Notebook 04** for:\n",
    "- Compare LSTM vs GARCH forecasts\n",
    "- Volatility targeting backtests\n",
    "- Statistical significance tests\n",
    "- Final conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later use\n",
    "from src.utils import save_model_checkpoint\n",
    "\n",
    "checkpoint_path = MODELS_DIR / f'lstm_{ticker}_{datetime.now().strftime(\"%Y%m%d\")}.pkl'\n",
    "\n",
    "save_model_checkpoint(\n",
    "    model_state=trainer.model.state_dict(),\n",
    "    filepath=checkpoint_path,\n",
    "    metadata={\n",
    "        'ticker': ticker,\n",
    "        'n_features': len(feature_cols),\n",
    "        'features': feature_cols,\n",
    "        'seq_len': LSTM_SEQ_LEN,\n",
    "        'hidden': LSTM_HIDDEN,\n",
    "        'layers': LSTM_LAYERS,\n",
    "        'val_loss': trainer.best_state['val_loss'] if hasattr(trainer, 'best_state') else None\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"✓ Model saved to {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
